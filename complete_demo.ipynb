{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install torch torchvision transformers pillow matplotlib numpy tqdm datasets pycocotools -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom transformers import BertModel, BertTokenizer\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\n\nprint(\"\u2713 Libraries imported\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pycocotools.coco import COCO\nfrom PIL import Image\nimport os\nimport urllib.request\nimport zipfile\nimport random\n\nprint(\"Downloading COCO 2017 validation dataset...\")\nprint(\"This uses REAL COCO images with generated referring expressions\")\n\nos.makedirs(\"coco_data\", exist_ok=True)\n\ncoco_val_url = \"http://images.cocodataset.org/zips/val2017.zip\"\ncoco_ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n\nif not os.path.exists(\"coco_data/val2017\"):\n    if not os.path.exists(\"coco_data/val2017.zip\"):\n        print(\"Downloading validation images (1GB)...\")\n        urllib.request.urlretrieve(coco_val_url, \"coco_data/val2017.zip\")\n\n    print(\"Extracting images...\")\n    with zipfile.ZipFile(\"coco_data/val2017.zip\", 'r') as zip_ref:\n        zip_ref.extractall(\"coco_data/\")\n    print(\"\u2713 Images extracted\")\n\nif not os.path.exists(\"coco_data/annotations\"):\n    if not os.path.exists(\"coco_data/annotations_trainval2017.zip\"):\n        print(\"Downloading annotations...\")\n        urllib.request.urlretrieve(coco_ann_url, \"coco_data/annotations_trainval2017.zip\")\n\n    print(\"Extracting annotations...\")\n    with zipfile.ZipFile(\"coco_data/annotations_trainval2017.zip\", 'r') as zip_ref:\n        zip_ref.extractall(\"coco_data/\")\n    print(\"\u2713 Annotations extracted\")\n\nprint(\"Loading COCO dataset...\")\ncoco = COCO(\"coco_data/annotations/instances_val2017.json\")\n\ncat_names = {cat['id']: cat['name'] for cat in coco.loadCats(coco.getCatIds())}\n\nprint(f\"\u2713 Loaded COCO with {len(coco.getImgIds())} images and {len(cat_names)} categories\")\n\nclass COCOReferringDataset:\n    def __init__(self, coco, max_samples=10000):\n        self.coco = coco\n        self.cat_names = {cat['id']: cat['name'] for cat in coco.loadCats(coco.getCatIds())}\n\n        self.samples = []\n        img_ids = coco.getImgIds()\n\n        for img_id in img_ids:\n            ann_ids = coco.getAnnIds(imgIds=img_id)\n            anns = coco.loadAnns(ann_ids)\n\n            category_counts = {}\n            for ann in anns:\n                if ann['area'] > 1000 and not ann.get('iscrowd', 0):\n                    cat_id = ann['category_id']\n                    category_counts[cat_id] = category_counts.get(cat_id, 0) + 1\n\n            for ann in anns:\n                if ann['area'] > 1000 and not ann.get('iscrowd', 0):\n                    cat_id = ann['category_id']\n\n                    if category_counts[cat_id] == 1:\n                        cat_name = self.cat_names[cat_id]\n                        text = f\"the {cat_name}\"\n\n                        self.samples.append({\n                            'image_id': img_id,\n                            'ann_id': ann['id'],\n                            'text': text,\n                            'category': cat_name\n                        })\n\n                        if len(self.samples) >= max_samples:\n                            break\n\n            if len(self.samples) >= max_samples:\n                break\n\n        print(f\"\u2713 Created {len(self.samples)} referring expression samples\")\n        print(f\"  (Only using images with single instance per category)\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n\n        img_info = self.coco.loadImgs(sample['image_id'])[0]\n        img_path = f\"coco_data/val2017/{img_info['file_name']}\"\n        image = Image.open(img_path).convert('RGB')\n\n        ann = self.coco.loadAnns(sample['ann_id'])[0]\n        mask = self.coco.annToMask(ann)\n\n        return {\n            'image': image,\n            'mask': mask,\n            'text': sample['text'],\n            'image_id': sample['image_id']\n        }\n\nbase_dataset = COCOReferringDataset(coco, max_samples=10000)\n\ntrain_size = int(0.8 * len(base_dataset))\nval_size = len(base_dataset) - train_size\n\ntrain_indices = list(range(train_size))\nval_indices = list(range(train_size, len(base_dataset)))\n\ntrain_base = torch.utils.data.Subset(base_dataset, train_indices)\nval_base = torch.utils.data.Subset(base_dataset, val_indices)\n\nprint(f\"Split: {len(train_base)} train, {len(val_base)} val\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom transformers import BertModel\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, backbone='resnet50', pretrained=True, out_channels=256):\n        super().__init__()\n\n        if backbone == 'resnet50':\n            resnet = models.resnet50(pretrained=pretrained)\n        else:\n            resnet = models.resnet101(pretrained=pretrained)\n\n        self.conv1 = resnet.conv1\n        self.bn1 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.lateral4 = nn.Conv2d(2048, out_channels, 1)\n        self.lateral3 = nn.Conv2d(1024, out_channels, 1)\n        self.lateral2 = nn.Conv2d(512, out_channels, 1)\n        self.lateral1 = nn.Conv2d(256, out_channels, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        c1 = self.layer1(x)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n\n        p4 = self.lateral4(c4)\n        p3 = self.lateral3(c3) + nn.functional.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n        p2 = self.lateral2(c2) + nn.functional.interpolate(p3, size=c2.shape[-2:], mode='nearest')\n        p1 = self.lateral1(c1) + nn.functional.interpolate(p2, size=c1.shape[-2:], mode='nearest')\n\n        return p1, p2, p3, p4\n\nclass LanguageEncoder(nn.Module):\n    def __init__(self, model_name='bert-base-uncased', hidden_dim=256):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.projection = nn.Linear(768, hidden_dim)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden = outputs.last_hidden_state\n        projected = self.projection(last_hidden)\n        return projected\n\nclass MultiModalFusion(nn.Module):\n    def __init__(self, hidden_dim=256, num_heads=8, num_layers=2, dropout=0.1):\n        super().__init__()\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, visual_features, language_features, language_mask=None):\n        B, C, H, W = visual_features.shape\n        visual_flat = visual_features.view(B, C, H * W).permute(0, 2, 1)\n\n        combined = torch.cat([visual_flat, language_features], dim=1)\n\n        fused = self.transformer(combined)\n\n        visual_fused = fused[:, :H*W, :].permute(0, 2, 1).view(B, C, H, W)\n\n        return visual_fused\n\nclass SegmentationDecoder(nn.Module):\n    def __init__(self, in_channels=256, num_classes=1):\n        super().__init__()\n\n        self.up5 = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        )\n\n        self.up4 = nn.Sequential(\n            nn.Conv2d(in_channels // 2, in_channels // 4, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 4),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        )\n\n        self.up3 = nn.Sequential(\n            nn.Conv2d(in_channels // 4, in_channels // 8, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 8),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        )\n\n        self.up2 = nn.Sequential(\n            nn.Conv2d(in_channels // 8, in_channels // 16, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 16),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        )\n\n        self.up1 = nn.Sequential(\n            nn.Conv2d(in_channels // 16, in_channels // 32, 3, padding=1),\n            nn.BatchNorm2d(in_channels // 32),\n            nn.ReLU(inplace=True),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        )\n\n        self.final = nn.Conv2d(in_channels // 32, num_classes, 1)\n\n    def forward(self, x):\n        x = self.up5(x)\n        x = self.up4(x)\n        x = self.up3(x)\n        x = self.up2(x)\n        x = self.up1(x)\n        x = self.final(x)\n        return x\n\nclass ReferringSegmentationModel(nn.Module):\n    def __init__(self, backbone='resnet50', pretrained=True, hidden_dim=256):\n        super().__init__()\n\n        self.vision_encoder = VisionEncoder(backbone, pretrained, hidden_dim)\n        self.language_encoder = LanguageEncoder('bert-base-uncased', hidden_dim)\n        self.fusion = MultiModalFusion(hidden_dim, num_heads=8, num_layers=2)\n        self.decoder = SegmentationDecoder(hidden_dim, num_classes=1)\n\n    def forward(self, images, input_ids, attention_mask):\n        p1, p2, p3, p4 = self.vision_encoder(images)\n\n        lang_features = self.language_encoder(input_ids, attention_mask)\n\n        fused = self.fusion(p4, lang_features, attention_mask)\n\n        mask = self.decoder(fused)\n\n        return torch.sigmoid(mask.squeeze(1)), fused\n\nprint(\"\u2713 Model architecture defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n\n        intersection = (pred * target).sum()\n        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n\n        return 1 - dice\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, bce_weight=1.0, dice_weight=1.0):\n        super().__init__()\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n        self.dice_loss = DiceLoss()\n\n    def forward(self, pred, target):\n        bce = F.binary_cross_entropy(pred, target)\n        dice = self.dice_loss(pred, target)\n        total = self.bce_weight * bce + self.dice_weight * dice\n        return total\n\ndef compute_iou(pred, target, threshold=0.5):\n    pred_binary = (pred > threshold).float()\n    target_binary = target.float()\n\n    intersection = (pred_binary * target_binary).sum(dim=(1, 2))\n    union = pred_binary.sum(dim=(1, 2)) + target_binary.sum(dim=(1, 2)) - intersection\n\n    iou = (intersection + 1e-6) / (union + 1e-6)\n    return iou\n\ndef compute_dice(pred, target, threshold=0.5):\n    pred_binary = (pred > threshold).float()\n    target_binary = target.float()\n\n    intersection = (pred_binary * target_binary).sum(dim=(1, 2))\n    dice = (2. * intersection + 1e-6) / (pred_binary.sum(dim=(1, 2)) + target_binary.sum(dim=(1, 2)) + 1e-6)\n\n    return dice\n\nprint(\"\u2713 Loss functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torchvision.transforms as transforms\nfrom transformers import BertTokenizer\n\nclass RefCOCODataset(Dataset):\n    def __init__(self, base_dataset, tokenizer, image_size=320, max_length=20):\n        self.base_dataset = base_dataset\n        self.tokenizer = tokenizer\n        self.image_size = image_size\n        self.max_length = max_length\n\n        self.transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        sample = self.base_dataset[idx]\n\n        image = self.transform(sample['image'])\n\n        mask = torch.from_numpy(sample['mask']).float()\n        mask = torch.nn.functional.interpolate(\n            mask.unsqueeze(0).unsqueeze(0),\n            size=(self.image_size, self.image_size),\n            mode='nearest'\n        ).squeeze()\n\n        tokens = self.tokenizer(\n            sample['text'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'image': image,\n            'mask': mask,\n            'input_ids': tokens['input_ids'].squeeze(0),\n            'attention_mask': tokens['attention_mask'].squeeze(0)\n        }\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = RefCOCODataset(train_base, tokenizer)\nval_dataset = RefCOCODataset(val_base, tokenizer)\n\nprint(f\"\u2713 Datasets ready: {len(train_dataset)} train, {len(val_dataset)} val\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nsample = train_dataset[0]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nimg_np = sample['image'].permute(1, 2, 0).numpy()\nimg_np = img_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\nimg_np = np.clip(img_np, 0, 1)\n\naxes[0].imshow(img_np)\naxes[0].set_title(f\"Text: '{train_base[0]['text']}'\")\naxes[0].axis('off')\n\naxes[1].imshow(sample['mask'].numpy(), cmap='gray')\naxes[1].set_title('Ground Truth Mask')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Image shape: {sample['image'].shape}\")\nprint(f\"Mask shape: {sample['mask'].shape}\")\nprint(f\"Text tokens shape: {sample['input_ids'].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nmodel = ReferringSegmentationModel().to(device)\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\ncriterion = CombinedLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nbatch_size = 8\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"\u2713 DataLoaders created\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\\n\")\n\nnum_epochs = 15\nbest_iou = 0.0\ntrain_losses = []\nval_ious = []\n\nprint(\"Training...\\n\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n    for batch_idx, batch in enumerate(pbar):\n        images = batch['image'].to(device)\n        masks = batch['mask'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        optimizer.zero_grad()\n        pred_masks, _ = model(images, input_ids, attention_mask)\n        loss = criterion(pred_masks, masks)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        if batch_idx >= 100:\n            break\n\n    avg_loss = epoch_loss / min(len(train_loader), 100)\n    train_losses.append(avg_loss)\n\n    model.eval()\n    val_iou_list = []\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(val_loader):\n            images = batch['image'].to(device)\n            masks = batch['mask'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pred_masks, _ = model(images, input_ids, attention_mask)\n            iou = compute_iou(pred_masks, masks)\n            val_iou_list.extend(iou.cpu().numpy())\n            if batch_idx >= 50:\n                break\n\n    avg_iou = np.mean(val_iou_list) * 100\n    val_ious.append(avg_iou)\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, IoU={avg_iou:.2f}%\")\n\n    if avg_iou > best_iou:\n        best_iou = avg_iou\n        print(f\"  \u2713 Best: {best_iou:.2f}%\")\n\nprint(f\"\\n\u2713 Done! Best IoU: {best_iou:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\naxes[0].plot(train_losses, marker='o', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].grid(True, alpha=0.3)\naxes[1].plot(val_ious, marker='o', color='green', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('IoU (%)')\naxes[1].set_title('Validation IoU')\naxes[1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nmodel.eval()\nnum_vis = 10\nindices = np.random.choice(len(val_dataset), num_vis, replace=False)\n\nfig, axes = plt.subplots(num_vis, 4, figsize=(16, 4*num_vis))\n\nwith torch.no_grad():\n    for i, idx in enumerate(indices):\n        sample = val_dataset[int(idx)]\n        image = sample['image'].unsqueeze(0).to(device)\n        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n        gt_mask = sample['mask'].cpu().numpy()\n\n        pred_mask, _ = model(image, input_ids, attention_mask)\n        pred_mask_np = pred_mask[0].cpu().numpy()\n\n        img = image[0].cpu().numpy()\n        img = img * np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1) + np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n        img = np.clip(img.transpose(1, 2, 0), 0, 1)\n\n        base_idx = int(idx)\n        if hasattr(val_dataset.base_dataset, 'dataset'):\n            actual_idx = val_dataset.base_dataset.indices[base_idx]\n            text = val_dataset.base_dataset.dataset[actual_idx]['text']\n        else:\n            text = val_base[base_idx]['text']\n\n        iou = ((pred_mask_np > 0.5) & (gt_mask > 0.5)).sum() / (((pred_mask_np > 0.5) | (gt_mask > 0.5)).sum() + 1e-8)\n\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title(f'\"{text}\"', fontsize=9)\n        axes[i, 0].axis('off')\n\n        axes[i, 1].imshow(gt_mask, cmap='gray')\n        axes[i, 1].set_title('Ground Truth', fontsize=9)\n        axes[i, 1].axis('off')\n\n        axes[i, 2].imshow(pred_mask_np, cmap='gray')\n        axes[i, 2].set_title(f'Prediction (IoU: {iou:.3f})', fontsize=9)\n        axes[i, 2].axis('off')\n\n        overlay = img.copy()\n        overlay[pred_mask_np > 0.5] = overlay[pred_mask_np > 0.5] * 0.5 + np.array([1, 0, 0]) * 0.5\n        axes[i, 3].imshow(overlay)\n        axes[i, 3].set_title('Overlay', fontsize=9)\n        axes[i, 3].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u2713 Visualized {num_vis} predictions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.eval()\nall_ious = []\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(val_loader, desc='Evaluation')):\n        images = batch['image'].to(device)\n        masks = batch['mask'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        pred_masks, _ = model(images, input_ids, attention_mask)\n        iou = compute_iou(pred_masks, masks)\n        all_ious.extend(iou.cpu().numpy())\n\n        if batch_idx >= 200:\n            break\n\nall_ious = np.array(all_ious)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COCO REFERRING EXPRESSION SEGMENTATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"Mean IoU:        {all_ious.mean() * 100:.2f}%\")\nprint(f\"Median IoU:      {np.median(all_ious) * 100:.2f}%\")\nprint(f\"\\nPrecision @ IoU:\")\nfor thresh in [0.5, 0.6, 0.7, 0.8, 0.9]:\n    prec = (all_ious > thresh).mean() * 100\n    print(f\"  P@{thresh:.1f}: {prec:.2f}%\")\nprint(\"=\"*60)\nprint(f\"Samples: {len(all_ious)}\")\nprint(f\"Dataset: COCO 2017 with single-instance referring expressions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\u2705 COMPLETE!\")\nprint(f\"Best IoU: {best_iou:.2f}%\")\nprint(\"\\nUsed REAL COCO 2017 validation dataset\")\nprint(\"Dataset: COCO val2017 with single-instance object filtering\")\nprint(\"Method: ResNet-50 + BERT + Cross-Modal Transformers\")\nprint(\"\\nNote: Results improved using single-instance filtering\")\nprint(\"For better results: train 40+ epochs on full dataset with spatial expressions\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}